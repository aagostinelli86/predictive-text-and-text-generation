{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive text and text generation\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "This notebook is a whirlwind tour of how certain kinds of predictive text generation work! By \"predictive text generation\" what I mean is any text generation method that is based around a statistical model that, given a certain stretch of text, \"predicts\" which bit of text should come next, based on probabilities learned from an existing corpus of text.\n",
    "\n",
    "The code is written in Python, but you don't really need to know Python in order to use the notebook. Everything's pre-written for you, so you can just execute the cells, making small changes to the code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text files\n",
    "\n",
    "Before we get started, we'll first need some text! Grab two [plain text files from Project Gutenberg](http://www.gutenberg.org/) (or from another source of your choice) and save them to the same directory as this notebook. (I suggest working with two files because we'll be running some code explicitly to \"compare\" two texts. Also, I think seeing two different outputs from the text generation methods discussed in this notebook will help you better understand how those methods work.) The code in the following cell loads into Python variables the contents of *two plain text files*, assigned to variables `text_a` and `text_b`. You'll need to replace the filenames with the names of the files that you downloaded, keeping the quotation marks (`\"`) intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_a = open(\"1342-0.txt\").read()\n",
    "text_b = open(\"84-0.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are *strings*, which are essentially just long lists of the characters that occur in the text, in the order that they occur. The code in the following cell shows the first two hundred characters of text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away \n"
     ]
    }
   ],
   "source": [
    "print(text_a[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change `text_a` to `text_b` to see the output from your second text, or change `200` to a number of your choosing.\n",
    "\n",
    "The `random.sample()` function gives us a random sampling of the contents of a variable (as long as that variable is a sequence of things, like a string or a list). So, for example, to see twenty random characters from text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 'y',\n",
       " 't',\n",
       " ' ',\n",
       " 's',\n",
       " 'i',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'o',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 'e']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(text_b, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't incredibly helpful on its own, but you'll notice that the characters it drew (probably) more or less follow the expected letter distribution for English (i.e., lots of `e`s and `n`s and `t`s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps more interesting would be to see a randomly-sampled list of *words*. To do this, we'll make separate variables for the words in the text, using a Python function called `.split()`, which takes a string and turns it into a list of words contained in that string. The following cell makes two new variables that contain the words from both texts respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_words = text_a.split()\n",
    "b_words = text_b.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ten random words from both text A and text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'arts',\n",
       " 'the',\n",
       " 'everything,',\n",
       " 'but',\n",
       " 'man-servant,',\n",
       " 'that',\n",
       " 'if',\n",
       " 'you',\n",
       " 'other']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(a_words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“William,',\n",
       " 'had',\n",
       " 'but',\n",
       " 'rose',\n",
       " 'of',\n",
       " 'the',\n",
       " 'endowed',\n",
       " 'up',\n",
       " 'me,',\n",
       " 'men;']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(b_words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell uses Python's `Counter` object to count the *most common* letters in the first of these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 113941),\n",
       " ('e', 70344),\n",
       " ('t', 47283),\n",
       " ('a', 42156),\n",
       " ('o', 41138),\n",
       " ('n', 38430),\n",
       " ('i', 36273),\n",
       " ('h', 33883),\n",
       " ('r', 33293),\n",
       " ('s', 33292),\n",
       " ('d', 22247),\n",
       " ('l', 21282)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(text_a).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the `a_words` variable gives the most frequent *words* instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4205),\n",
       " ('to', 4121),\n",
       " ('of', 3662),\n",
       " ('and', 3309),\n",
       " ('a', 1945),\n",
       " ('her', 1858),\n",
       " ('in', 1813),\n",
       " ('was', 1795),\n",
       " ('I', 1740),\n",
       " ('that', 1419),\n",
       " ('not', 1356),\n",
       " ('she', 1306)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(a_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to the most common words in text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4056),\n",
       " ('and', 2971),\n",
       " ('of', 2741),\n",
       " ('I', 2719),\n",
       " ('to', 2142),\n",
       " ('my', 1631),\n",
       " ('a', 1394),\n",
       " ('in', 1125),\n",
       " ('was', 993),\n",
       " ('that', 987),\n",
       " ('with', 700),\n",
       " ('had', 679)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(b_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov models: what comes next?\n",
    "\n",
    "Now that we have the ability to find and record the n-grams in a text, it’s time to take our analysis one step further. The next question we’re going to try to answer is this: Given a particular n-gram in a text, what is most likely to come next?\n",
    "\n",
    "We can imagine the kind of algorithm we’ll need to extract this information from the text. It will look very similar to the code to find n-grams above, but it will need to keep track not just of the n-grams but also a list of all units (word, character, whatever) that *follow* those n-grams.\n",
    "\n",
    "Let’s do a quick example by hand. This is the same character-level order-2 n-gram analysis of the (very brief) text “condescendences” as above, but this time keeping track of all characters that follow each n-gram:\n",
    "\n",
    "| n-grams |\tnext? |\n",
    "| ------- | ----- |\n",
    "|co| n|\n",
    "|on| d|\n",
    "|nd| e, e|\n",
    "|de| s, n|\n",
    "|es| c, (end of text)|\n",
    "|sc| e|\n",
    "|ce| n, s|\n",
    "|en| d, c|\n",
    "|nc| e|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can determine that while the n-gram `co` is followed by n 100% of the time, and while the n-gram `on` is followed by `d` 100% of the time, the n-gram `de` is followed by `s` 50% of the time, and `n` the rest of the time. Likewise, the n-gram `es` is followed by `c` 50% of the time, and followed by the end of the text the other 50% of the time.\n",
    "\n",
    "Exercise: Imagine (or even better, write out) what this table might look like if you were analyzing words instead of characters, with a source text of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chains: Generating text from a Markov model\n",
    "\n",
    "The Markov models we created above don't just give us interesting statistical probabilities. It also allows us generate a *new* text with those probabilities by *chaining together predictions*. Here’s how we’ll do it, starting with the order 2 character-level Markov model of `condescendences`: (1) start with the initial n-gram (`co`)—those are the first two characters of our output. (2) Now, look at the last *n* characters of output, where *n* is the order of the n-grams in our table, and find those characters in the “n-grams” column. (3) Choose randomly among the possibilities in the corresponding “next” column, and append that letter to the output. (Sometimes, as with `co`, there’s only one possibility). (4) If you chose “end of text,” then the algorithm is over. Otherwise, repeat the process starting with (2). Here’s a record of the algorithm in action:\n",
    "\n",
    "    co\n",
    "    con\n",
    "    cond\n",
    "    conde\n",
    "    conden\n",
    "    condend\n",
    "    condendes\n",
    "    condendesc\n",
    "    condendesce\n",
    "    condendesces\n",
    "    \n",
    "As you can see, we’ve come up with a word that looks like the original word, and could even be passed off as a genuine English word (if you squint at it). From a statistical standpoint, the output of our algorithm is nearly indistinguishable from the input. This kind of algorithm—moving from one state to the next, according to a list of probabilities—is known as a Markov chain generator.\n",
    "\n",
    "### Generating with Markovify\n",
    "\n",
    "Fortunately, with the invention of digital computers, you don't have to perform this algorithm by hand! In fact, Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. It comes with a lot of extra niceties that will make our lives easier, but underneath the hood, it implements an algorithm very similar to the one we just did by hand above.\n",
    "\n",
    "To install Markovify on your computer, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in /Users/allison/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: unidecode in /Users/allison/anaconda/lib/python3.6/site-packages (from markovify)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run this cell to make the library available in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `generator_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then call the `.make_sentence()` method to generate a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But at last very rapidly, as well for the present, and, as they wished.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balls will be married!\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were few people of fashion.”\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by disabling the check altogether with `test_output=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Mr. and Mrs. Long.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the order\n",
    "\n",
    "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
    "gen_a_4 = markovify.Text(text_a, state_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1\n",
      "Not to utter more especially commercial redistribution.\n",
      "\n",
      "order 4\n",
      "However little Mr. Darcy might have done as well.”\n"
     ]
    }
   ],
   "source": [
    "print(\"order 1\")\n",
    "print(gen_a_1.make_sentence(test_output=False))\n",
    "print()\n",
    "print(\"order 4\")\n",
    "print(gen_a_4.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the level\n",
    "\n",
    "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
    "\n",
    "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_model = SentencesByChar(\"condescendences\", state_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condendendescescendesces'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_char = SentencesByChar(text_a, state_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could rest her eloping and took off my glove, and has since reached me two or three younger girls, too eager to offer to her head.\n"
     ]
    }
   ],
   "source": [
    "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models\n",
    "\n",
    "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)\n",
    "generator_b = markovify.Text(text_b)\n",
    "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bit of code `[0.5, 0.5]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a *soupçon* of text B, you would write `[0.9, 0.1]`. Try it!) \n",
    "\n",
    "Then you can create sentences using the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accordingly brought him from the observation of a little paler than usual, but more frequently sunk me into one of distinguishing the ladies both of whom we shall be the purport of this speech convinced my father said, with the former.\n"
     ]
    }
   ],
   "source": [
    "print(combo.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "I've pre-written some code below to make it easy for you to experiment and produce output from Markovify. Just make adjustments to the values assigned to the variables in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change to \"word\" for a word-level model\n",
    "level = \"char\"\n",
    "# controls the length of the n-gram\n",
    "order = 7\n",
    "# controls the number of lines to output\n",
    "output_n = 14\n",
    "# weights between the models; text A first, text B second.\n",
    "# if you want to completely exclude one model, set its corresponding value to 0\n",
    "weights = [0.5, 0.5]\n",
    "# limit sentence output to this number of characters\n",
    "length_limit = 280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The lines beginning with `#` are \"comments\"—they don't do anything, they're just there to explain what's happening in the code.)\n",
    "\n",
    "After making your changes above, run the cell below to generate text according to your parameters. Repeat as necessary until you get something you really like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of neither from you paid the formerly beheld my endeavour to hear such a case.”\n",
      "\n",
      "Jane was the rewarded and as Elizabeth longed to know the occasionally to her having events.\n",
      "\n",
      "He has been commit my thoughts and thus traverse the pleasure and we returned in so dear to feel how unequalled.\n",
      "\n",
      "Mr. Wickham immediately invited to the gratify whatever of honoured father would she had saved a human nature or pain was allayed by her complaints, and prepared forth in paragraph 1.F.3, a full discussed.\n",
      "\n",
      "How can it be so unequal to the gaieties of activity in the fidgets.\n",
      "\n",
      "What made her turn without some minutes, she read it like to give an accompany her uncle and altogether, soon went forgot.\n",
      "\n",
      "“My fingers,” said to her sister's room, she soon joined by the wanted in my last look perfect and openly acknowledge is an advanced.\n",
      "\n",
      "Yet he entertained by the inn.\n",
      "\n",
      "He did not at all possible, white steeple and elevating her judgment for free      electronic work be imperfect in society of them, would be made; it is like me.\n",
      "\n",
      "Mont Blanc, and I recollection of your without the induce me to something new acquaintances.\n",
      "\n",
      "He is determined what effect purpose, but from me, if not the merits; and if she had ever believe, very ill Miss King is wounding moderate!\n",
      "\n",
      "She longer.\n",
      "\n",
      "Of neither choice?\n",
      "\n",
      "What he should be always operation is nothing of Darcy's eyes wandered me unsocial; but otherwise have determined to Colonel Forster?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(text_a, state_size=order)\n",
    "gen_b = model_cls(text_b, state_size=order)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
    "for i in range(output_n):\n",
    "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
    "    out = out.replace(\"\\n\", \" \")\n",
    "    print(out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're comparing here is called *unigram frequency*. (\"Unigram\" means a sequence of length one—more on this below.) For most English texts, the most frequent words in any given text will correspond closely to the most common "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network text prediction with `textgenrnn`\n",
    "\n",
    "Like a [Markov chain](ngrams-and-markov-chains.ipynb), a recurrent neural network (RNN) is a way to make predictions about what will come next in a sequence. For our purposes, the sequence in question is a sequence of characters, and the prediction we want to make is *which character will come next*. Both Markov models and recurrent neural networks do this by using statistical properties of text to make a *probability distribution* for what character will come next, given some information about what comes before. The two procedures work very differently internally, and we're not going to go into the gory details about implementation here. (But if you're interested in the gory details, [here's a good place to start](https://karpathy.github.io/2015/05/21/rnn-effectiveness/).) For our purposes, the main *functional* difference between a Markov chain and a recurrent neural network is the *portion* of the sequence used to make the prediction. A Markov model uses a fixed window of history from the sequence, while an RNN (theoretically) uses the *entire history* of the sequence.\n",
    "\n",
    "## Start with Markov\n",
    "\n",
    "To illustrate, here's that Markov model of the word \"condescendences.\" In a Markov model based on bigrams from this string of characters, you'd make a list of bigrams and the characters that follow those bigrams, like so:\n",
    "\n",
    "| n-grams |\tnext? |\n",
    "| ------- | ----- |\n",
    "| co      | n |\n",
    "| on      | d |\n",
    "| nd      | e, e |\n",
    "| de      | s, n |\n",
    "| es      | c, (end of text) |\n",
    "| sc      | e |\n",
    "| ce      | n, s |\n",
    "| en      | d, c |\n",
    "| nc      | e |\n",
    "\n",
    "You could also write this as a probability distribution, with one column for each bigram. The value in each cell indicates the probability that the character following the bigram in a given row will be followed by the character in a given column:\n",
    "\n",
    "| n-grams | c | o | n | d | e | s | END |\n",
    "| ------- | - | - | - | - | - | - | --- |\n",
    "| co      | 0 | 0 | 1.0 | 0 | 0 | 0 | 0 | \n",
    "| on      | 0 | 0 | 0 | 1.0 | 0 | 0 | 0 | \n",
    "| nd      | 0 | 0 | 0 | 0 | 1.0 | 0 | 0 | \n",
    "| de      | 0 | 0 | 0.5 | 0 | 0 | 0.5 | 0 |\n",
    "| es      | 0.5 | 0 | 0 | 0 | 0 | 0 | 0.5 |\n",
    "| sc      | 0 | 0 | 0 | 0 | 1.0 | 0 | 0 |\n",
    "| ce      | 0 | 0 | 0.5 | 0 | 0 | 0.5 | 0 |\n",
    "| en      | 0.5 | 0 | 0 | 0.5 | 0 | 0 | 0 |\n",
    "| nc      | 0 | 0 | 0 | 0 | 1.0 | 0 | 0 |\n",
    "\n",
    "Each row of this table is a *probability distribution*, meaning that it shows how probable a given letter is to follow the n-gram in the original text. In a probability distribution, all of the values add up to 1.\n",
    "\n",
    "Fitting a Markov model to the data is a matter of looking at each sequence of characters in a given text, and updating the table of probability distributions accordingly. To make a prediction from this table, you can \"sample\" from the probability distribution for a given n-gram (i.e., sampling from the distribution for the bigram `de`, you'd have a 50% chance of picking `n` and a 50% chance of picking `s`).\n",
    "\n",
    "Another way of thinking about this Markov model is as a (hypothetical!) function *f* that takes a bigram as a parameter and returns a probability distribution for that bigram:\n",
    "\n",
    "    f(\"ce\") → [0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0]\n",
    "    \n",
    "(Note that the values at each index in this distribution line up with the columns in the table above.)\n",
    "    \n",
    "The items in the list returned from this function correspond to the probability for the corresponding next character, as given in the table. To sample from this list, you'd pick randomly among the indices according to their probabilities, and then look up the corresponding character by its position in the table.\n",
    "\n",
    "To generate new text from this model:\n",
    "\n",
    "1. Set your output string to a randomly selected n-gram\n",
    "2. Sample a letter from the probability distribution associated with the n-gram at the end of the output string\n",
    "3. Append the sampled letter to the end of the string\n",
    "4. Repeat from (2) until the END token is reached\n",
    "\n",
    "Of course, you don't write this function by hand! When you're creating a Markov model from your data (or \"training\" the model), you're essentially asking the computer to write this function *for you*. In this sense, a Markov model is a very simple kind of machine learning, since the computer \"learns\" the probability distribution from the data that you feed it.\n",
    "\n",
    "## A (very) simplified explanation of RNNs\n",
    "\n",
    "The mechanism by which a recurrent neural network \"learns\" probability distributions from sequences is much more sophisticated than the mechanism used in a Markov model, but functionally they're very similar: you give the computer some data to \"train\" on, and then ask it to automatically create a function that will return a probability distribution of what comes next, given some input. An RNN differs from a Markov chain in that to predict the next item in the sequence, you pass in *the entire sequence* instead of just the most recent n-gram.\n",
    "\n",
    "In other words, you can (again, hypothetically) think of an RNN as a way of automatically creating a function *f* that takes a sequence of characters of arbitrary length and returns a probability distribution for which character comes next in the sequence. Unlike a Markov chain, it's possible to *improve* the accuracy of the probability distribution returned from this function by training on the same data multiple times.\n",
    "\n",
    "Let's say that we want to train the RNN on the string \"condescendences\" to learn this function, and we want to make a prediction about what character is most likely to follow the sequence of characters \"condescendence\". When training a neural network, the process of learning a function like this works iteratively: you start off with a function that essentially gives a uniform probability distribution for each outcome (i.e., no one outcome is considered more likely than any other):\n",
    "\n",
    "    f(\"condescendences\") → [0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14] (after zero passes through the data)\n",
    "    \n",
    "... and as you iterate over the training data (in this case, the word \"condescendences\"), the probability distribution  gradually improves, ideally until it comes to accurately reflect the actual observed distribution (in the parlance, until it \"converges\"). After some number of passes through the data, you might expect the automatically-learned function to return distributions like this:\n",
    "\n",
    "    f(\"condescendences\") → [0.01, 0.02, 0.01, 0.03, 0.01, 0.9, 0.02] (after n passes through the data)\n",
    "\n",
    "A single pass through the training data is called an \"epoch.\" When it comes to any neural network, and RNNs in particular, more epochs is almost always better.\n",
    "\n",
    "To generate text from this model:\n",
    "\n",
    "1. Initialize your output string to an empty string, or a random character, or a starting \"prefix\" that you specify;\n",
    "2. Sample the next letter from the distribution returned for the current output string;\n",
    "3. Append that character to the end of the output string;\n",
    "4. Repeat from (2)\n",
    "\n",
    "Of course, in a real life application of both a Markov model and an RNN, you'd normally have more than seven items in the probability distribution! In fact, you'd have one element in the probability distribution for every possible character that occurs in the text. (Meaning that if there were 100 unique characters found in the text, the probability distribution would have 100 items in it.)\n",
    "\n",
    "## Markov chains vs RNNs    \n",
    "\n",
    "The primary benefit of an RNN over a Markov model for text generation is that an RNN takes into account *the entire history* of a sequence when generating the next character. This means that, for example, an RNN can theoretically learn how to close quotes and parentheses, which a Markov chain will never be able to reliably do (at least for pairs of quotes and parentheses longer than the n-gram of the Markov chain).\n",
    "\n",
    "The drawback of RNNs is that they are *computationally expensive*, from both a processing and memory perspective. This is (again) a simplification, but internally, RNNs work by \"squishing\" information about the training data down into large matrices, and make predictions by performing calculations on these large matrices. That means that you need a lot of CPU and RAM to train an RNN, and the resulting models (when stored to disk) can be very large. Training an RNN also (usually) takes a lot of time.\n",
    "\n",
    "Another consideration is the size of your corpus. Markov models will give interesting and useful results even for very small datasets, but RNNs require large amounts of data to train—the more data the better.\n",
    "\n",
    "So what do you do if you *don't* have a very large corpus? Or if you don't have a lot of time to train on your corpus?\n",
    "\n",
    "## RNN generation from pre-trained models\n",
    "\n",
    "Fortunately for us, developer and data scientist [Max Woolf](https://github.com/minimaxir) has made a Python library called [textgenrnn](https://github.com/minimaxir/textgenrnn) that makes it really easy to experiment with RNN text generation. This library includes a model (according to the documentation) \"trained on hundreds of thousands of text documents, from Reddit submissions (via BigQuery) and Facebook Pages (via my Facebook Page Post Scraper), from a very diverse variety of subreddits/Pages,\" and allows you to use this model as a starting point for your own training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install textgenrnn with `pip`. (You don't need to do this if you're running the notebook on Binder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textgenrnn in /Users/allison/anaconda/lib/python3.6/site-packages\n",
      "Requirement already up-to-date: h5py in /Users/allison/anaconda/lib/python3.6/site-packages (from textgenrnn)\n",
      "Requirement already up-to-date: keras>=2.1.5 in /Users/allison/anaconda/lib/python3.6/site-packages (from textgenrnn)\n",
      "Requirement already up-to-date: scikit-learn in /Users/allison/anaconda/lib/python3.6/site-packages (from textgenrnn)\n",
      "Requirement already up-to-date: six in /Users/allison/anaconda/lib/python3.6/site-packages (from h5py->textgenrnn)\n",
      "Requirement already up-to-date: numpy>=1.7 in /Users/allison/anaconda/lib/python3.6/site-packages (from h5py->textgenrnn)\n",
      "Requirement already up-to-date: keras-applications>=1.0.6 in /Users/allison/anaconda/lib/python3.6/site-packages (from keras>=2.1.5->textgenrnn)\n",
      "Requirement already up-to-date: scipy>=0.14 in /Users/allison/anaconda/lib/python3.6/site-packages (from keras>=2.1.5->textgenrnn)\n",
      "Requirement already up-to-date: keras-preprocessing>=1.0.5 in /Users/allison/anaconda/lib/python3.6/site-packages (from keras>=2.1.5->textgenrnn)\n",
      "Requirement already up-to-date: pyyaml in /Users/allison/anaconda/lib/python3.6/site-packages (from keras>=2.1.5->textgenrnn)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade textgenrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's installed, import the `textgenrnn` class from the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a new `textgenrnn` object like so. (The `name` parameter controls the filename used when automatically saving the model to disk, so pick something descriptive!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textgen = textgenrnn(name=\"text_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has a `.generate()` method which will, by default, generate text from the pre-trained model only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A stream in his boys and they came out or something but any time whenever you have any street start in the start?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `textgenrnn` library needs a data structure called a \"list of strings\" as its source text for training. We'll use Markovify's `split_into_sentences` method to turn our plain-text input files into lists of sentences like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from markovify.splitters import split_into_sentences\n",
    "text_a_sentences = split_into_sentences(text_a)\n",
    "text_b_sentences = split_into_sentences(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random sentences from both texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“But I was embarrassed.”',\n",
       " 'She does not yet leave her dressing-room.',\n",
       " 'This event had at last been despaired of, but it was then\\ntoo late to be saving.',\n",
       " '“I am by no means of the opinion, I assure you,” said he, “that a ball\\nof this kind, given by a young man of character, to respectable people,\\ncan have any evil tendency; and I am so far from objecting to dancing\\nmyself, that I shall hope to be honoured with the hands of all my fair\\ncousins in the course of the evening; and I take this opportunity of\\nsoliciting yours, Miss Elizabeth, for the two first dances especially,\\na preference which I trust my cousin Jane will attribute to the right\\ncause, and not to any disrespect for her.”',\n",
       " 'I wonder what he can be doing there.”']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(text_a_sentences, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The light of that conflagration\\nwill fade away; my ashes will be swept into the sea by the winds.',\n",
       " 'Years will pass, and\\nyou will have visitings of despair and yet be tortured by hope.',\n",
       " 'My hand was already on the\\nlock of the door before I recollected myself.',\n",
       " 'Ever since the fatal night, the end of my labours, and the\\nbeginning of my misfortunes, I had conceived a violent antipathy even\\nto the name of natural philosophy.',\n",
       " 'The\\ndissecting room and the slaughter-house furnished many of my materials;\\nand often did my human nature turn with loathing from my occupation,\\nwhilst, still urged on by an eagerness which perpetually increased, I\\nbrought my work near to a conclusion.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(text_b_sentences, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a text generator on your own text, use the `.train_on_texts()` method, passing in a list of strings. The `num_epochs` parameter allows you to indicate how many epochs (i.e., passes over the data) should be performed. The more epochs the better, especially for shorter texts, but you'll get okay results even with just a few.\n",
    "\n",
    "Training a neural network usually takes a really long time! So it makes sense to \"try out\" a text before committing to the many hours it might take to train the network on the full text. The following example trains the neural network on just the first 100 lines from text A, which lets you get an idea of what the output will look like when training on its entire contents. You'll notice that the `train_on_texts()` function prints output as it goes, showing what the generated text is likely to look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 8,345 character sequences.\n",
      "Epoch 1/3\n",
      "65/65 [==============================] - 28s 433ms/step - loss: 2.0301\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "“What the share is a single thing to have his that the share of the menthoo my dear.\n",
      "\n",
      "“What is the suphed to the share of the sisters.”\n",
      "\n",
      "“I have no used to he have no one of the share to he want to she she she she she want to she she has some of the wife is a silly.”\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "“What and sayche that you heard than the end of the sission so invision to you have you say the for the sissive with that and the amutagies it and so that the execrapation to have said has no only have so her than the morning.”\n",
      "\n",
      "“I was some of the medichion what you will to have has onlation me in my dear.\n",
      "\n",
      "“Prepe has he desig want to see the sandon today only have no replied the wife to deserve you send to not the dear that he deserve his the thing me.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Winding dird slaphing feels; I have insigrhing includent on Goodbleishhers in secret,” says,unversing and ms and her neut vighain in only afer raised beforlant should semath.”\n",
      "\n",
      "“You do to do Ama our leofishming acrent.”\n",
      "\n",
      "“I neighbed with execrate to he bel but not notap.\n",
      "\n",
      "Epoch 2/3\n",
      "65/65 [==============================] - 27s 408ms/step - loss: 1.3906\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "“I have no one of the song to be a single thing of the she was to the such a single thing to his no one of the such a single thing to the more of the single tim thing that the said the single thing to the said that the share that the she are you will think of the single thing to the single normal o\n",
      "\n",
      "“I do not to he want to take this will the such a how thing to his not the so them so you and in the for the consideroous as you must say them.\n",
      "\n",
      "“I am so the surrount of the said that the single tick of the said that the said them so you will be any ked themselves that he was thing to the surrount and not the such not the single thing to the said them, and in the said it is the single thing to send to the single normal and the more of the s\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "“I have a single tim that you will day you may do not to and she share in the for the for my dear, but that them, Mr. But the last in the for intervision that a come and many his that the army of them give thems on this in the morning of the last to take these in the consideroous with it no one of \n",
      "\n",
      "“I have no origing to mean in the beauty to thing to his sisters will be cases and married her and my disting is think to you are that I should not send it a nerve of the for the first to serve it.\n",
      "\n",
      "“What may affect though thing my disconnill will be a good or thing for the dear, that ones of the view of them, her seasour assure of the man is to know will not not know what it is them?”\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "“The acquision who losed a bet north was more going to you! on the user classion yousure not was like thow mirther.\n",
      "\n",
      "The Understatoon Art Lill Engo, Un: his actoures.\n",
      "\n",
      "“By the city; a send at causing themriced to expessed you murt men my difficults.”\n",
      "\n",
      "Epoch 3/3\n",
      "65/65 [==============================] - 41s 634ms/step - loss: 1.1574\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "“I have not to have a sology and in the said the she are you may such a how the man and in the surround of the handsome of the fortune of the man of the surrount that he had he was a single thing of the surrounding of the terms of the marrying that I have a sourth than the surrount of the surrount \n",
      "\n",
      "“I do not to heard that he had not cannot dear.\n",
      "\n",
      "“I am so has a single thing of the sure of the first to said her to the little as you must that he want to know when I have not to be a man of the surrounding of the since is a nerve of the she was that he will be any ked that he was a hairchmand to heard to be any kerapee on the said the surrouse \n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "“I have no, by the share at their estally cannot have to have a little so familied to he is it at the girls of my daughters.\n",
      "\n",
      "“I am so much twing of them, you cough the girls than them art in the heard in the end of the menth and new not to make that can you have anyone any kevt her wind of them to dear Mr. Bennet,” though introduce in the captain when the forth to hearth a such introduce and many really must some of them\n",
      "\n",
      "“I cannot will be any ideas my day of the fortune for your introduction abused; and the next tim lizzard; my dear Mr. Bingley.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "“What arealls; ifned happail of youfou williest and thave, what has information?”\n",
      "\n",
      "“You may was a fornocant contain free?”\n",
      "\n",
      "“I made his fill Bingless, but benn prepersion and manymimmus is notingsome, for tears, fiveawalls is aber, abulation on his lady, you such a winter!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.train_on_texts(text_a_sentences[:100], num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can generate new text using the `.generate()` method again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have a single play for the suite of the she will _l_.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't very interesting because by default the generator is very conservative in how it samples from the probability distribution. You can use the `temperature` parameter to make the sampling a bit more likely to pick improbable outcomes. The higher the value, the weirder the results. The default is 0.2, and going above 1.0 is likely to produce unacceptably strange results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have no one of the thing and some of Mrs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But it was finland end this a girls; it will be is deal his how much to informed her employed ahmedy it?”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RiteMTHr since 22,senses: Lengun of R,nowcets, fone therliefiу?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(temperature=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass a number `n` to the `.generate()` method as its first parameter, `.generate()` will print out `n` instances of text generation from the model. The code in the following cell prints out ten examples from the specified temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:28,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I am sure to have a good termary in you when married use of the her intervice of the silly to make the surround of the fortune of the more of the surforthlowless will be must enough to have surved him to the meaning of the way to heard them?”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:04<00:22,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“My encode of the she difficult into the rest as a guybou and information in them in the formand is a she indenem.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:06<00:16,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have not been for intervery to heard less for my dear, and so belraged; “with it was all them, for sendendablum.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:10<00:16,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I am serist is the tim twisty of them like that I am sure you accused; that he is an account, that they she was a termade, and must such a such a single her come of set man of the handsome to dear, “he had do, what you must she was better.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:12<00:13,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Post A Mr. Bennet,” the man will be in a formand of the sure in the said the she consider wheelifffort when he had are you are a neighbourhood with her assure, “on the best of cast took and is a consider.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:15<00:10,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Do you think of your her sending to take the dear married in her nite and in a silly that he was the has she had; a single daughters.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:17<00:07,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I honour his so much timmey, that he can a silly anyone have heard of them termin to the such tree this winter.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:21<00:06,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“When a posstee of Mrs Lizzy is for the formand introduction, that he was so this will tell you he came it on the man of AustenPromoda and Mr. Bennet and chance, at some of the she dear, that you he was a way to be seen so much to come on her in the more of think to be such a formand competion scol\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:26<00:03,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have not she want to have a visit of this she will dear and many he cannot depresive of my little waited with the half on the wife who he was thinking to come on the her to see any she was to be a shirt and waited the sureself in the girls now.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:28<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have a was thereforretic the herself in the forse in a handsome of his a way that you know with you may sure it intended themselves now.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(10, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This may take a little while.)\n",
    "\n",
    "If you specify `return_as_list=True`, the `.generate()` method returns the results as a list instead of printing them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“When the such in one was not must sure it.”', 'I was so the streament in a settling it intended; that you must was that he wished; that he can he was a news of Mr. Bennet may so you will be any cannabu when can north them, and not make that you must know what we decimie and it is see it and it is any keve_ it any know themselves you amand in th', '“Does not that he was married them, and advice it in the her song.', '“I have a single tim for him to her and it taken his dear do you must think of the considers.”', '“But you have anything .', '“I hope a man of so finte them all of my dear, what you think of the said that I have to know what you the house of introduce of them so you you must make an alway and her wife.”', '“I have a balture of the his not depended with that I am sure if searched and therefore is a father.', 'A was theother and it is addrick them and anyone any keve_ her more of the married than the hand of herself so went in the hand to make to get ney with himmyyones are of my dear, so your dear, and single let you have it anyone anywhere any keve_ the such a half her intervettes; and Lizzy as a siste', '“I have no restriction when a set in a formand to him no use them, Mrs.', '“I honour them so you see a serve of the visit of the formand better.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated_strs = textgen.generate(10, temperature=0.5, return_as_list=True)\n",
    "print(generated_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're satisfied with the results and you're ready to train on all of the sentences, just remove the `[:100]` from the call to `.train_on_texts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textgen.train_on_texts(text_a_sentences, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textgenrnn library automatically saves the model to disk after each epoch in the same directory as this notebook. You can load a model you've previously trained by passing its filename to the `textgenrnn` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textgen = textgenrnn(\"text_a_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can call the `.generate()` method as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:27,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How make that see the such a said my best of the sister will be in a single will never to be the thing than them as you have a good term of they will be caperad in the for them, sick and her she been with Mrs.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:03<00:18,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have a wife are consider you may be abluse?”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:07<00:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I was a man abused to be anything who are all them, little assure it wouldned; “the only is to heard to see the surrouse in the such a way of Mr. Bennet,” and the she had; it is introducedolic and anyone the married of the such as a not must any changeland it insurention them.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:08<00:13,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You cannot she should sending to sendenberg used to him to take nervous.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:12<00:13,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I do not want to see the grandic will tell you the his own what that I have an are a gight be on the mother has a dear, be muchdely that he have an not so have some of not the fortune of the beauty to start when like that I am heard; that it is a have served to actually will deall of entither.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:13<00:08,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have a nerve of the onlymores are wife.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:14<00:06,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“But he she will not water _unter to see how much themsight to watche them said to you should be not won them.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:19<00:05,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I have no used on the first of the sued of the sisters the wife had no also be replied that he is not must have a gue of the heaven to take the prepast on the amuse of the serios to find that it is to the heavy includedwiffling to she she designg the such she is no married; I was prepeyd and is a \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:20<00:02,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Are you may was tour to be so muchdeliess them.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:21<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I said that I was so devereed to heard or so, that he can be one of them.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(10, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*Note*: If you're running this on Binder, make sure to download the weights from the notebook server's Home page! You can upload them again when you start a new session. Binder will automatically delete the data associated with inactive notebooks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating with shorter texts\n",
    "\n",
    "I've found that `textgenrnn` works especially well with very short, word-length texts. For example, download [this file of human moods](https://github.com/dariusk/corpora/blob/master/data/humans/moods.json) from Corpora Project, and put it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the JSON file and grab just the list of words naming moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "mood_data = json.loads(open(\"./moods.json\").read())\n",
    "moods = mood_data['moods']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create another textgenrnn object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mood_gen = textgenrnn(name=\"moods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the RNN on these moods. One epoch will do the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 6,651 character sequences.\n",
      "Epoch 1/1\n",
      "51/51 [==============================] - 23s 442ms/step - loss: 2.2959\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "art\n",
      "\n",
      "derressed\n",
      "\n",
      "instanted\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "inactly\n",
      "\n",
      "alliee\n",
      "\n",
      "tarreated\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "deed\n",
      "\n",
      "tymed\n",
      "\n",
      "joygucture\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mood_gen.train_on_texts(moods, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a list of new moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:00<00:02,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owned\n",
      "\n",
      "innepsess\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:00<00:02,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insterette\n",
      "\n",
      "resepted\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:00<00:02,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joyals\n",
      "\n",
      "indderful\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [00:00<00:02,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cored\n",
      "\n",
      "jusital\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:01<00:01,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selful\n",
      "\n",
      "aimed\n",
      "\n",
      "friend\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:01<00:01,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suppleased\n",
      "\n",
      "exielshed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:01<00:01,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sitterful\n",
      "\n",
      "indenden\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [00:02<00:01,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriss\n",
      "\n",
      "erlssepredy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [00:02<00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depire\n",
      "\n",
      "contime\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [00:02<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanted\n",
      "\n",
      "exploured\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [00:02<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feated\n",
      "\n",
      "belsiffful\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 25/25 [00:03<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "\n",
      "volutic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mood_gen.generate(25, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [This notebook from the creator of textgenrnn](https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb) covers everything about the library that I covered in this tutorial—and much more, including how to start generation from a particular \"seed\" and how to save and load models (useful if you spent an afternoon training a model on your own corpus and don't want to have to do it again!)\n",
    "* Take a look at [Janelle Shane's wonderful overview of how she uses RNNs in her process](http://aiweirdness.com/faq). And then take a look at her [wonderful creative work with RNNs](http://aiweirdness.com/).\n",
    "* Hayes, Brian. “Computer recreations.” Scientific American, vol. 249, no. 5, 1983, pp. 18–31. JSTOR, http://www.jstor.org/stable/24969024. (Original column from Scientific American that described how Markov chain text generation works—very readable! I can send a PDF, hit me up.)\n",
    "* [A Travesty Generator for Micros](https://elmcip.net/critical-writing/travesty-generator-micros) is a follow-up to Hayes' article that has some more theory and an actual Pascal listing (which is now mostly of only historical interest).\n",
    "* [This notebook](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) shows how to implement a Markov chain generator from scratch in Python, if you're interested in such things!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
